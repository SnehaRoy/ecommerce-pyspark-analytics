# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/

# Jupyter
.ipynb_checkpoints/
*.ipynb_checkpoints

# Data files (don't commit large datasets)
data/raw/*.csv
data/raw/*.json
data/processed/
*.parquet

# Spark
derby.log
metastore_db/
spark-warehouse/

# IDE
.vscode/
.idea/
*.swp

# OS
.DS_Store
Thumbs.db
EOF

# Create initial project structure
mkdir -p {data/{raw,processed,output},notebooks,src/{ingestion,transformation,analytics,utils},config,tests,docs}

# Create README
cat > README.md << EOF
# E-commerce Analytics with PySpark

## Project Description
End-to-end data analytics pipeline for e-commerce data using Apache Spark.

## Features
- Distributed data processing with PySpark
- Customer segmentation and RFM analysis
- Product performance analytics
- Real-time streaming simulation
- SCD Type 2 implementation
- Performance optimization techniques

## Tech Stack
- PySpark 3.4.0
- Python 3.8+
- Jupyter Notebooks
- Parquet/Delta Lake

## Project Structure
\`\`\`
ecommerce-pyspark-analytics/
├── data/
│   ├── raw/           # Raw data files
│   ├── processed/     # Processed data
│   └── output/        # Analysis results
├── notebooks/         # Jupyter notebooks
├── src/
│   ├── ingestion/     # Data loading modules
│   ├── transformation/# ETL logic
│   ├── analytics/     # Analytics modules
│   └── utils/         # Helper functions
├── config/            # Configuration files
├── tests/             # Unit tests
└── docs/              # Documentation
\`\`\`

## Setup Instructions
[To be added]

## Usage
[To be added]
EOF
